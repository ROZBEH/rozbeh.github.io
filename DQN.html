<!DOCTYPE html>
<html>
<head><meta charset="utf-8" />
<title>colab_101</title>

<script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML' async></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.1.10/require.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>
<link rel="stylesheet" href="//fonts.googleapis.com/css?family=Roboto:300,300italic,700,700italic">
<link rel="stylesheet" href="//cdn.rawgit.com/necolas/normalize.css/master/normalize.css">
<link rel="stylesheet" href="//cdn.rawgit.com/milligram/milligram/master/dist/milligram.min.css">



<body>
<p>In this post I am going to talk about Deap Q Learning (DQN). The main idea is based on two papers that were published in 2013 and 2015 by Google Deepmind. The <a href="https://arxiv.org/pdf/1312.5602.pdf">firt one</a> is a NIPS paper and lets call it DQN1. The <a href="https://storage.googleapis.com/deepmind-media/dqn/DQNNaturePaper.pdf">second paper</a> is a nature paper and we call it DQN2 in this post. </br>
DQN1: Playing Atari with Deep Reinforcement Learning</br>
DQN2: Human-level control through deep reinforcement learning </br>

The main idea of these two papers is to be able to beat the computer at Atari games with no human intervention. 

The papers combine ideas from reinforcement learning (Q-learning) and deep learning (CNN) in order to achieve the performance.

First let's talk about Q-learning. Q-learning is a dynamic programming approach for updating our estimation about agent's future success. In Q-learning agents experience consists of a sequence of distinct stages or episodes. In the \(n^{th}\) episode, agent: </br>

Observes its current state \(x_n\) </br>
Selects and performs an action \(a_n\) </br>
Observes the subsequenct state \(x_{n+1}\) </br>
Adjusts its Q<sub>n-1</sub> value using the learning factor \(\alpha_{n}\) according to </br>





\[
{Q_{n}(x,a)} =
\left\{
\begin{array}{
  @{}% no padding
  l@{\quad}% some padding
  r@{}% no padding
  >{{}}r@{}% no padding
  >{{}}l@{}% no padding
}
  (1-\alpha_{n})Q_{n-1}(x,a) + [r + \gamma V_{n-1}(x_{n+1}) ]& \\
  Q_{n-1}(x,a) & terminal &state
\end{array}
\right.
\]

in which \(V_{n-1}(x_{n+1})\) is the best possible value that the agent can achieve from state \(x_{n+1}\) on ward, in other words <b>expected future reward</b>.
$$V_{n-1}(x_{n+1}) = \max_{b} {Q_{n-1}(x_{n+1},b)}$$ 

\(\alpha_n\) is the learning factor </br>

If \(\alpha_n = 0\), agent does not learn and exploits prior information.</br>
If \(\alpha_n = 1\), agent learns new things and explres the environment.</br>










</br>



</p>
	
</body>
</html>
