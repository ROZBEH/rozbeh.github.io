<!DOCTYPE html>
<html>
<head><meta charset="utf-8" />
<title>colab_101</title>

<script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML' async></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.1.10/require.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>
<link rel="stylesheet" href="//fonts.googleapis.com/css?family=Roboto:300,300italic,700,700italic">
<link rel="stylesheet" href="//cdn.rawgit.com/necolas/normalize.css/master/normalize.css">
<link rel="stylesheet" href="//cdn.rawgit.com/milligram/milligram/master/dist/milligram.min.css">



<body>
<p>In this post I am going to talk about Deap Q Learning (DQN). The main idea is based on two papers that were published in 2013 and 2015 by Google Deepmind. The <a href="https://arxiv.org/pdf/1312.5602.pdf">firt one</a> is a NIPS paper and lets call it DQN1. The <a href="https://storage.googleapis.com/deepmind-media/dqn/DQNNaturePaper.pdf">second paper</a> is a nature paper and we call it DQN2 in this post. </br>
DQN1: Playing Atari with Deep Reinforcement Learning</br>
DQN2: Human-level control through deep reinforcement learning </br>

The main idea of these two papers is to be able to beat the computer at Atari games with no human intervention. 

The papers combine ideas from reinforcement learning (Q-learning) and deep learning (CNN) in order to achieve the performance.

First let's talk about Q-learning. Q-learning is a dynamic programming approach for updating our estimation about agent's future success. In Q-learning agents experience consists of a sequence of distinct stages or episodes. In the n<sup>th</sup> episode, agent: </br>

Observes its current state \(x_n\) </br>
Selects and performs an action \(a_n\) </br>
Observes the subsequenct state \(x_(n+1)\) </br>
Adjusts its Q<sub>n-1</sub> value using the learning factor &alpha;<sub>n</sub> according to </br>




</p>
	
</body>
</html>
