<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.8.0 -->
<title>Deep Reinforcement Learning: DQN | Rouzbeh Shirvani</title>
<meta name="generator" content="Jekyll v4.3.2" />
<meta property="og:title" content="Deep Reinforcement Learning: DQN" />
<meta name="author" content="rozbeh" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Deep Reinforcement Learning: DQN" />
<meta property="og:description" content="Deep Reinforcement Learning: DQN" />
<meta property="og:site_name" content="Rouzbeh Shirvani" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2023-12-19T06:36:56+00:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Deep Reinforcement Learning: DQN" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","author":{"@type":"Person","name":"rozbeh"},"dateModified":"2023-12-19T06:36:56+00:00","datePublished":"2023-12-19T06:36:56+00:00","description":"Deep Reinforcement Learning: DQN","headline":"Deep Reinforcement Learning: DQN","mainEntityOfPage":{"@type":"WebPage","@id":"/private/2023/12/19/deep-reinforcement-learning-DQN.html"},"url":"/private/2023/12/19/deep-reinforcement-learning-DQN.html"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/private/assets/css/style.css"><link type="application/atom+xml" rel="alternate" href="/feed.xml" title="Rouzbeh Shirvani" />
</head>
<body><header class="site-header">

  <div class="wrapper"><a class="site-title" rel="author" href="/">Rouzbeh Shirvani</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/about/">About</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Deep Reinforcement Learning: DQN</h1>
    <p class="post-meta"><time class="dt-published" datetime="2023-12-19T06:36:56+00:00" itemprop="datePublished">
        Dec 19, 2023
      </time></p>
  </header>

  <div class="post-content e-content" itemprop="articleBody">
    <p>In this post I am going to talk about Deep Q Learning (DQN). The main idea is based 
on two papers that were published in 2013 and 2015 by Google Deepmind. 
The first one<sup id="fnref:1" role="doc-noteref"><a href="#fn:1" class="footnote" rel="footnote">1</a></sup> is a NIPS paper, lets call it DQN1 and the second paper<sup id="fnref:2" role="doc-noteref"><a href="#fn:2" class="footnote" rel="footnote">2</a></sup> is a nature paper and we call it DQN2 in this post.</p>

<p>DQN1: Playing Atari with Deep Reinforcement Learning
DQN2: Human-level control through deep reinforcement learning</p>

<p>In order to make it easier for the general reader to follow, this post contains sub-posts that will dive deeper into the subject. This post is also accompanied by the code snippet of the TensorFlow implementation of each part.</p>

<p>The main idea of these two papers is to be able to beat the computer at Atari games with no minimum supervision.</p>

<p>The papers combine ideas from reinforcement learning (Q-learning) and deep learning (CNN) in order to achieve the performance.
I also wrote short post about Q-learning. Please go to <a href="#q-learning">appendix</a> if you want to learn more about it. In summary, Q-learning is a dynamic programming approach for updating our estimation about agent’s future success.</p>

<p>One thing that comes very handy in reinforcement learning is the Bellman Equation. Bellman equation is a way of discounted future reward.</p>

\[R_t = r_t + \gamma (r_{t+1} + \gamma ( r_{t+2} + ...)) = r_t + \gamma R_{t+1}\]

<p>Since we use $Q$ in order to express expected future reward, we will have:</p>

\[Q(s_t, a) = \max_{\pi}  R_{t+1}\]

<p>$\pi(s) = \argmax_{a} Q(s, a)$ is the policy that leads to the best reward $Q(s,a)$.</p>

<p>For simple transition ${&lt;}s_t,a,r,s_{t+1}{&gt;}$, the bellman equation would be:</p>

\[Q(s_t,a) = r + \gamma \max_{a'} Q(s_{t+1},a')\]

<p>In the <a href="https://arxiv.org/pdf/1312.5602.pdf">DQN1 paper</a> that was published in 2013, a convolutional neural network (CNN) is used as a function to estimate $Q$. The actual reward $r$ at each time step is being returned by the simulation environment, which the authors call an emulator.</p>

<p>The overall workflow of the DQN consists of two major part. Here we will cover each part in detail:</p>

<h2 id="part-1">Part 1</h2>

<p>This part has to do with the memory. Suppose the agent is at state $s_t$. The state is 
the images pixels of the Atari game. The take 4 consecutive frames and stack them over each
other and consider that as $s_t$. $s_t$ will be fed into the CNN. It will output multiple
$Q$ values corresponding to each action. For example, if a game has 4 different possible 
actions then the network will output 4 different $Q$ values corresponding to each action.
Now, the action corresponding to the highest $Q$ values will be stored as $a$. $a$
will be fed to the emulator and the emulator will return the reward $r$ and the next 
state $s_{t+1}$. Now $(s_t, a, r, s_{t+1})$ will be stored into the memory.</p>

<p><img src="/private/assets/img/2023-12-18-deep-reinforcement-learning-DQN/netQ.png" alt="Network Q" height="258" width="505" /></p>

\[Q_n(x, a) =
\begin{cases}
(1 - \alpha_n)Q_{n-1}(s_t, a) + [\text{r} + \gamma V_{n-1}(s_{t+1})] &amp; \text{if not terminal state} \\
Q_{n-1}(s_t, a) &amp; \text{if terminal state}
\end{cases}\]

<h2 id="part-2">Part 2</h2>

<p>In the $2^{nd}$ part network learns and updates its parameters. Here something very interesting happens. As the authors point out in both <a href="https://arxiv.org/pdf/1312.5602.pdf">DQN1</a> and <a href="https://storage.googleapis.com/deepmind-media/dqn/DQNNaturePaper.pdf">DQN2</a> papers, the network is not trained based on the current state of the game. That’s why their approach is off-policy. They train the network by taking a random sample $(s_t, a, r, s_{t+1})$ from memory. If you want to know why, please go to the <a href="#experience-replay">appendix</a> section.</p>

<p>Once we sample $(s_t, a, r, s_{t+1})$ from memory, $s_{t+1}$ will be fed into the network and network will output all possible $Q$ values but we choose the one with the highest value and call it $q’$. This value will be used to calculate the target action values $Target_q = r + \gamma q’ $. Now, we have to estimate our prediction about action value $Q$. In order to do so, we pass $s_{t}$ to the network and the network will output $Q$ for all possible actions. We pick the one that corresponds to the action $a$ and call it $Prediction_q = r + \gamma q’ $. At the end backpropagation happens on this cost function:</p>

\[loss = (Target_q - Prediction_q)^2\]

<!-- <script src="https://gist.github.com/ROZBEH/123b32d27374dd4a4b3958d6bd32d365.js"></script> -->

<p>Note that we have two types of predictions. The first one happens when we are storing it in memory from some episodes in the past, the one that we pass $s_t$ to the memory. The second one happens during training when we sample from memory and we pass $s_t$ and $s_{t+1}$ in order to calculate $Prediction_q$ and $target_q$ respectively.</p>

<p>Here I try to answer some of the questions that I originally had while reading DQN papers:</p>

<p>I. What is the states $s$: It is raw image pixels, sequence of $k$ frames. This way we can consider temporal differences as well as being able to play roughly $k$ more times.</p>

<p>II. What is output of the neural network: Output of the neural network is the value function estimating future reward $R_t$ at time $t$ or $Q_t$.</p>

<p>III. What is experience reply: In short, experience reply refers to times when we don’t use the most recent observations in order to update the weights rather we sample from memory and update the weights based on this sample.</p>

<p>As we said before, $Q$ follows the bellman equation:</p>

\[Q^*(s,a) = \max_{\pi} \mathbb{E}\{R_t|S_t=s, A_t=a,\pi\} = \mathbb{E}_{s' \sim \epsilon} [r+\gamma \max_{a'} Q^*(s',a') | S=s,A=a]\]

<p>In which $\epsilon$ is the environment that the agent is interacting with.
Given that the optimal value of $Q^*(s’,a’)$ for the state $s’$ is known for all possible actions $a’$. In our case neural network will provide an estimate of possible values for all actions.</p>

<p>The loss function for the neural network is as follows: 
\(L_i(\theta) =  \mathbb{E}_{s,a \sim \rho(.) } [(y_i - Q(s,a;\theta_i))^2]\)
\(\text{Where: } y_i = \mathbb{E}_{s' \sim \epsilon} [r + \gamma max_{a'} Q(s',a';\theta_{i-1}) ]\)
Note that we are using $\theta_{i-1}$ in the notation for $y_i$ in order to update the weights of neural network. This is a very clever approach that they use in the paper. This makes the algorithm more stable. Generating targets using an older set of parameters adds a delay between the time an update to $Q$ is made and the time update affects the target $y_j$. This will make divergence and oscillation more unlikely. In other words predictions are based on parameters of a few steps back rather than the parameters that just have been updated.</p>

<p>Also, unlike supervised learning, here the target value depends on the weights of neural network. In supervised value the target value is known beforehand. Also, $s,a \sim \rho(.)$  is actually the replay memory that we sample from.</p>

<ul>
  <li>Actions are taken with $\epsilon$ greedy! Which means that the agent follows the greedy strategy with probability $(1-\epsilon)$ and the agent selects a random action with probability $\epsilon$ </li>
  <li>Q-learning is a model-free approach. Because we longer know the probability of state transition. If you need more information about DQN is model-free, please watch <a href="https://www.coursera.org/lecture/practical-rl/model-based-vs-model-free-MnfUn">this</a> video.</li>
  <li>Authors in <a href="https://arxiv.org/pdf/1312.5602.pdf">DQN1</a> paper show that there are two ways to show the progress of training. Average $Q$ value and average reward over the course of training. As it turns out, average $Q$ is a better measure of how the agent is learning.</li>
</ul>

<p>Here I am going to talk about a little bit of history. Before using CNN for estimating value function $Q$, there use to be a table for $Q$ in which rows were state and columns were actions.  The update process worked like this:</p>

\[Q_{i+1}(s,a) = \mathbb{E} [r + \gamma max_{a'} Q_{i}(s',a')|s,a ]\]

<p>$Q_i$ Approaches to the optimal value function as $i\rightarrow\infty$. 
The size of the table is $|s| \times |a| $ in which $|s|$ is the total number 
of states and $|a|$ is total number of possible actions that the agent can take. 
This table will get updated at each episode. Doing this is impractical due to the 
large number of states and instead we use a function approximator. In the case of 
DQN, we are using Deep NN in order to perform this where $Q_i(s’,a’)$ is estimated
by CNN and $Q_{i+1}$ is used in the loss function to backpropagate and update the weights.</p>

<p>The <a href="https://storage.googleapis.com/deepmind-media/dqn/DQNNaturePaper.pdf">DQN2</a> 
paper is almost the same as the first one with few differences that I list here:</p>

<ul>
	<li>In the second paper they add t-SNE visualization in order to show hidden representation of image frames. They show that similar image frames end up with similar 2-D representations.</li>
	<li>They use two different networks for target network and prediction network. The parameters of target networks are non-trainable but parameters of prediction network are trainable. At every $C$ iteration, parameters of target network are cloned from prediction network.</li>
</ul>

<p>As mentioned in DQN2 paper, one of the possible future areas of exploration 
could be choosing best memories and discarding memories that do not have sufficient 
information. Right now, they just override the oldest memories with the newest episodes.
What could be done differently? <a href="https://pdfs.semanticscholar.org/6086/ab80f6d2ba77dd7da65c3cadf8cf5fb019ae.pdf">
This</a> work could be a good starting point. One idea could be, after $M$ number of
episodes of training, we could use t-SNE or other dimensionality reduction algorithms
in order to remove similar memories and keep few of them instead of having a lot of
similar experiences.</p>

<h1 id="appendix">Appendix</h1>

<h2 id="experience-replay">Experience Replay</h2>

<p>In DQN papers, they train the network by taking a random sample $(s_t, a, r, s_{t+1})$ from memory. They are doing this for three main reasons.</p>

<p>a. Data efficiency</p>

<p>b. Learning directly from consecutive samples is inefficient due to correlation between samples</p>

<p>c. When learning on-policy, the current parameters determine the next data samples that parameters are trained on. For example, if the maximizing action is to move left, then training samples will be dominated by samples from left-hand side! This is called Experience Replay and it avoids diversion in parameters. The current parameters are different  from those used to generate samples because samples are taken from memory not current experience.</p>

<h2 id="q-learning">Q-learning</h2>

<p>Q-learning is a dynamic programming approach for updating our estimation about agent’s 
  future success. In Q-learning agents experience consists of a sequence of distinct stages 
  or episodes. At the $t^{th}$ times of the $n^{th}$ episode, agent:</p>

<p>Observes its current state $s_t$ 
Selects and performs an action $a_t$ 
Observes the subsequent state $s_{t+1}$ 
Adjusts its $Q_{n-1}$ value using the learning factor $\alpha_{n}$ according to the equation below:</p>

\[Q_{n}(s_t,a) =
\begin{cases}
(1-\alpha_{n})Q_{n-1}(s_t,a) + [r + \gamma V_{n-1}(s_{t+1})], &amp; \text{if not terminal state} \\
Q_{n-1}(s_t,a), &amp; \text{if terminal state}
\end{cases}\]

<p>In this equation $V_{n-1}(s_{t+1})$ is the best possible value that the agent can achieve from state $s_{t+1}$ on ward, in other words expected future reward.</p>

\[V_{n-1}(s_{t+1}) = \max_{b} {Q_{n-1}(s_{t+1},b)}\]

<p>$\alpha_n$ is the learning factor</p>

<p>If $\alpha_n = 0$, agent does not learn and exploits prior information.
If $\alpha_n = 1$, agent learns new things and explores the environment.</p>

<h1 id="references">References</h1>

<div class="footnotes" role="doc-endnotes">
  <ol>
    <li id="fn:1" role="doc-endnote">
      <p><a href="https://storage.googleapis.com/deepmind-media/dqn/DQNNaturePaper.pdf">Human-level control through deep reinforcement learning</a> <a href="#fnref:1" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:2" role="doc-endnote">
      <p><a href="https://arxiv.org/pdf/1312.5602.pdf">Playing Atari with Deep Reinforcement Learning</a> <a href="#fnref:2" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
  </ol>
</div>

  </div><a class="u-url" href="/private/2023/12/19/deep-reinforcement-learning-DQN.html" hidden></a>
</article>

<script src="https://giscus.app/client.js"
        data-repo="rozbeh/rozbeh.github.io"
        data-repo-id="MDEwOlJlcG9zaXRvcnkxMzE0MzkxNTk="
        data-category="Q&A"
        data-category-id="DIC_kwDOB9WaN84Cb4IS"
        data-mapping="pathname"
        data-strict="0"
        data-reactions-enabled="1"
        data-emit-metadata="0"
        data-input-position="bottom"
        data-theme="light"
        data-lang="en"
        crossorigin="anonymous"
        async>
</script>


      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/"></data>

  <div class="wrapper">

    <div class="footer-col-wrapper">
      <div class="footer-col">
        <ul class="contact-list">
          <li class="p-name">rozbeh</li>
          <li><a class="u-email" href="mailto:rouzbeh.asghari@gmail.com">rouzbeh.asghari@gmail.com</a></li>
        </ul>
      </div>
      <div class="footer-col">
        <p></p>
      </div>
    </div>

    <div class="social-links"><ul class="social-media-list"><li>
  <a rel="me" href="https://github.com/rozbeh" target="_blank" title="github">
    <svg class="svg-icon grey">
      <use xlink:href="/private/assets/minima-social-icons.svg#github"></use>
    </svg>
  </a>
</li>
<li>
  <a rel="me" href="https://www.linkedin.com/in/rozbeh" target="_blank" title="linkedin">
    <svg class="svg-icon grey">
      <use xlink:href="/private/assets/minima-social-icons.svg#linkedin"></use>
    </svg>
  </a>
</li>
<li>
  <a rel="me" href="https://x.com/rouzbehshirvani" target="_blank" title="x">
    <svg class="svg-icon grey">
      <use xlink:href="/private/assets/minima-social-icons.svg#x"></use>
    </svg>
  </a>
</li>
</ul>
</div>

  </div>

</footer>
</body>

</html>
<script src="/private/assets/js/codebutton.js"></script>
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
<script>
  window.MathJax = {
    tex: {
      inlineMath: [['$', '$'], ['\\(', '\\)']],
      macros: {
          argmax: "\\mathop{\\rm arg\\,max}\\nolimits"
        }
    }
  };
  </script>