<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.3.2">Jekyll</generator><link href="/feed.xml" rel="self" type="application/atom+xml" /><link href="/" rel="alternate" type="text/html" /><updated>2023-12-22T08:05:47+00:00</updated><id>/feed.xml</id><title type="html">Rouzbeh Shirvani</title><subtitle></subtitle><author><name>rozbeh</name><email>rouzbeh.asghari@gmail.com</email></author><entry><title type="html">Google Colab Introduction</title><link href="/private/2023/12/19/deep-Google-Colab-Introduction.html" rel="alternate" type="text/html" title="Google Colab Introduction" /><published>2023-12-19T06:36:56+00:00</published><updated>2023-12-19T06:36:56+00:00</updated><id>/private/2023/12/19/deep-Google-Colab-Introduction</id><content type="html" xml:base="/private/2023/12/19/deep-Google-Colab-Introduction.html"><![CDATA[<p>In this session we are going to talk about the basics Colab. These basics include:</p>
<ul>
  <li>Using Colab enivironment as command line</li>
  <li>Reading file from local google drive</li>
  <li>Reading file from your local machine</li>
  <li>Downloading file to your local machine!</li>
  <li>Using GPU!</li>
  <li>Notes!</li>
</ul>

<h2 id="using-colab-environment-as-command-line">Using Colab environment as command line</h2>

<p>By using excalamation mark in front of any command Colab will treat it as terminal command. For example one could use !ls to list the files in the current directory. Here are few examples:</p>

<ol>
  <li>!wget “http://datasets.d2.mpi-inf.mpg.de/MPIIGaze/MPIIGaze.tar.gz”
  for downloading the file</li>
  <li>!unzip myfile.zip
  for unzipping myfile.zip file</li>
  <li>!pip install -U -q PyDrive
  Installing Python libraries</li>
  <li>!apt-get install tar
  Installing command line dependencies</li>
  <li>!rm Test.py
  removing the file Test.py from server</li>
</ol>

<h2 id="reading-file-from-google-drive-into-colab-environment">Reading file from google drive into Colab environment</h2>

<p>If you are working with Python and you have a lot of files to deal with, it is more convinient to upload your files to Google Drive and read them from there. First you need to give access to Colab in order to read your files. I should also mention that Colab file should be stored in the same Google account as Google Drive. First use the following command to use the python utility for accessing Google drive!</p>

<p><code class="language-plaintext highlighter-rouge">!pip install -U -q PyDrive</code></p>

<p>Use the following code snippet in order to give access to your Google Drive.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>import os
from pydrive.auth import GoogleAuth
from pydrive.drive import GoogleDrive
from google.Colab import auth
from oauth2client.client import GoogleCredentials

auth.authenticate_user()
gauth = GoogleAuth()
gauth.credentials = GoogleCredentials.get_application_default()
drive = GoogleDrive(gauth)
</code></pre></div></div>

<p>Once you give access to Colab, now it’s time to specify the name of the folder that you want to read its containing files. For now we just focus on the files inside a folder NOT folders inside folder. The alphanumerical code like <code class="language-plaintext highlighter-rouge">1pqZR3iYuwvVAw6BhSXHymQysIQVG8bnb</code> specifies the link that shows in your browser when you are inside the specific folder. If you are inside a folder in Google Drive and look the browser URL, some codes like this will be at the end of URL.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>local_download_path = os.path.expanduser('~/data')
try:
  os.makedirs(local_download_path)
except: pass

file_list = drive.ListFile(
    {'q': "'1pqZR3iYuwvVAw6BhSXHymQysIQVG8bnb' in parents"}).GetList()
</code></pre></div></div>

<p>Once you read this specific folder, all the files will be stored inside file_list. In order to read some specific files you can commands like the following one. This command will download the files from your Google Drive to the local folder on server.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>for f in file_list:
  print('title: %s, id: %s' % (f['title'], f['id']))
  fname = os.path.join(local_download_path, f['title'])
  print('downloading to {}'.format(fname))
  f_ = drive.CreateFile({'id': f['id']})
  f_.GetContentFile(fname)
</code></pre></div></div>

<p>Once the file is downloaded to the local folder on server which is usually on a folder like <code class="language-plaintext highlighter-rouge">/content/data/</code></p>

<p>A little description about the following code snippet. Suppose we used the previous code snippet downloaded the <code class="language-plaintext highlighter-rouge">dev_brown.txt</code> file from Google Drive to local folder on server <code class="language-plaintext highlighter-rouge">/content/data/dev_brown.txt</code>, in order to read the file content and we just have to use the two following lines of python code.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>text_file = open ("/content/data/dev_brown.txt",'r')
lines = text_file.readlines()
</code></pre></div></div>

<h2 id="reading-file-from-your-local-machine-to-colab-environment">Reading file from your local machine to Colab environment</h2>

<p>If you have a file in your local machine and you want to upload it to the server local drive, use the following code snippet!</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>from google.Colab import files
uploaded = files.upload()
for fn in uploaded.keys():
  print('User uploaded file "{name}" with length {length} bytes'.format(
      name=fn, length=len(uploaded[fn])))
</code></pre></div></div>

<h2 id="downloading-file-to-your-local-machine">Downloading file to your local machine</h2>
<p>If you want to save some files from server into your local machine, use the following commands.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>from google.Colab import files
with open('example.txt', 'w') as f:
  f.write('some content')
files.download('example.txt')
</code></pre></div></div>

<h2 id="using-gpu">Using GPU!</h2>
<p>If you would like to use GPU in your Python code. Go to Runtime on the menu and choose <code class="language-plaintext highlighter-rouge">Change runtime type</code> and then choose GPU in the <code class="language-plaintext highlighter-rouge">Hardware accelerator</code>.</p>

<h2 id="notes">Notes</h2>

<ul>
  <li>
    <p>If you want to connect the Colab to your local machine, try to use this link.
https://research.google.com/colaboratory/local-runtimes.html</p>
  </li>
  <li>It is not to connect to a workspace on google Colab while another code is still running on the same work space! You should wait for the previous one to finish and then run your code!</li>
  <li>You have 12 hours to finish your task on Colab otherwise all of your files and results will be deleted.</li>
</ul>]]></content><author><name>rozbeh</name><email>rouzbeh.asghari@gmail.com</email></author><category term="private" /><summary type="html"><![CDATA[Google Colab Introduction]]></summary></entry><entry><title type="html">Deep Reinforcement Learning: DQN</title><link href="/private/2023/12/19/deep-reinforcement-learning-DQN.html" rel="alternate" type="text/html" title="Deep Reinforcement Learning: DQN" /><published>2023-12-19T06:36:56+00:00</published><updated>2023-12-19T06:36:56+00:00</updated><id>/private/2023/12/19/deep-reinforcement-learning-DQN</id><content type="html" xml:base="/private/2023/12/19/deep-reinforcement-learning-DQN.html"><![CDATA[<p>In this post I am going to talk about Deep Q Learning (DQN). The main idea is based 
on two papers that were published in 2013 and 2015 by Google Deepmind. 
The first one<sup id="fnref:1" role="doc-noteref"><a href="#fn:1" class="footnote" rel="footnote">1</a></sup> is a NIPS paper, lets call it DQN1 and the second paper<sup id="fnref:2" role="doc-noteref"><a href="#fn:2" class="footnote" rel="footnote">2</a></sup> is a nature paper and we call it DQN2 in this post.</p>

<p>DQN1: Playing Atari with Deep Reinforcement Learning
DQN2: Human-level control through deep reinforcement learning</p>

<p>In order to make it easier for the general reader to follow, this post contains sub-posts that will dive deeper into the subject. This post is also accompanied by the code snippet of the TensorFlow implementation of each part.</p>

<p>The main idea of these two papers is to be able to beat the computer at Atari games with no minimum supervision.</p>

<p>The papers combine ideas from reinforcement learning (Q-learning) and deep learning (CNN) in order to achieve the performance.
I also wrote short post about Q-learning. Please go to <a href="#q-learning">appendix</a> if you want to learn more about it. In summary, Q-learning is a dynamic programming approach for updating our estimation about agent’s future success.</p>

<p>One thing that comes very handy in reinforcement learning is the Bellman Equation. Bellman equation is a way of discounted future reward.</p>

\[R_t = r_t + \gamma (r_{t+1} + \gamma ( r_{t+2} + ...)) = r_t + \gamma R_{t+1}\]

<p>Since we use $Q$ in order to express expected future reward, we will have:</p>

\[Q(s_t, a) = \max_{\pi}  R_{t+1}\]

<p>$\pi(s) = \argmax_{a} Q(s, a)$ is the policy that leads to the best reward $Q(s,a)$.</p>

<p>For simple transition ${&lt;}s_t,a,r,s_{t+1}{&gt;}$, the bellman equation would be:</p>

\[Q(s_t,a) = r + \gamma \max_{a'} Q(s_{t+1},a')\]

<p>In the <a href="https://arxiv.org/pdf/1312.5602.pdf">DQN1 paper</a> that was published in 2013, a convolutional neural network (CNN) is used as a function to estimate $Q$. The actual reward $r$ at each time step is being returned by the simulation environment, which the authors call an emulator.</p>

<p>The overall workflow of the DQN consists of two major part. Here we will cover each part in detail:</p>

<h2 id="part-1">Part 1</h2>

<p>This part has to do with the memory. Suppose the agent is at state $s_t$. The state is 
the images pixels of the Atari game. The take 4 consecutive frames and stack them over each
other and consider that as $s_t$. $s_t$ will be fed into the CNN. It will output multiple
$Q$ values corresponding to each action. For example, if a game has 4 different possible 
actions then the network will output 4 different $Q$ values corresponding to each action.
Now, the action corresponding to the highest $Q$ values will be stored as $a$. $a$
will be fed to the emulator and the emulator will return the reward $r$ and the next 
state $s_{t+1}$. Now $(s_t, a, r, s_{t+1})$ will be stored into the memory.</p>

<p><img src="/private/assets/img/2023-12-18-deep-reinforcement-learning-DQN/netQ.png" alt="Network Q" height="258" width="505" /></p>

\[Q_n(x, a) =
\begin{cases}
(1 - \alpha_n)Q_{n-1}(s_t, a) + [\text{r} + \gamma V_{n-1}(s_{t+1})] &amp; \text{if not terminal state} \\
Q_{n-1}(s_t, a) &amp; \text{if terminal state}
\end{cases}\]

<h2 id="part-2">Part 2</h2>

<p>In the $2^{nd}$ part network learns and updates its parameters. Here something very interesting happens. As the authors point out in both <a href="https://arxiv.org/pdf/1312.5602.pdf">DQN1</a> and <a href="https://storage.googleapis.com/deepmind-media/dqn/DQNNaturePaper.pdf">DQN2</a> papers, the network is not trained based on the current state of the game. That’s why their approach is off-policy. They train the network by taking a random sample $(s_t, a, r, s_{t+1})$ from memory. If you want to know why, please go to the <a href="#experience-replay">appendix</a> section.</p>

<p>Once we sample $(s_t, a, r, s_{t+1})$ from memory, $s_{t+1}$ will be fed into the network and network will output all possible $Q$ values but we choose the one with the highest value and call it $q’$. This value will be used to calculate the target action values $Target_q = r + \gamma q’ $. Now, we have to estimate our prediction about action value $Q$. In order to do so, we pass $s_{t}$ to the network and the network will output $Q$ for all possible actions. We pick the one that corresponds to the action $a$ and call it $Prediction_q = r + \gamma q’ $. At the end backpropagation happens on this cost function:</p>

\[loss = (Target_q - Prediction_q)^2\]

<!-- <script src="https://gist.github.com/ROZBEH/123b32d27374dd4a4b3958d6bd32d365.js"></script> -->

<p>Note that we have two types of predictions. The first one happens when we are storing it in memory from some episodes in the past, the one that we pass $s_t$ to the memory. The second one happens during training when we sample from memory and we pass $s_t$ and $s_{t+1}$ in order to calculate $Prediction_q$ and $target_q$ respectively.</p>

<p>Here I try to answer some of the questions that I originally had while reading DQN papers:</p>

<p>I. What is the states $s$: It is raw image pixels, sequence of $k$ frames. This way we can consider temporal differences as well as being able to play roughly $k$ more times.</p>

<p>II. What is output of the neural network: Output of the neural network is the value function estimating future reward $R_t$ at time $t$ or $Q_t$.</p>

<p>III. What is experience reply: In short, experience reply refers to times when we don’t use the most recent observations in order to update the weights rather we sample from memory and update the weights based on this sample.</p>

<p>As we said before, $Q$ follows the bellman equation:</p>

\[Q^*(s,a) = \max_{\pi} \mathbb{E}\{R_t|S_t=s, A_t=a,\pi\} = \mathbb{E}_{s' \sim \epsilon} [r+\gamma \max_{a'} Q^*(s',a') | S=s,A=a]\]

<p>In which $\epsilon$ is the environment that the agent is interacting with.
Given that the optimal value of $Q^*(s’,a’)$ for the state $s’$ is known for all possible actions $a’$. In our case neural network will provide an estimate of possible values for all actions.</p>

<p>The loss function for the neural network is as follows: 
\(L_i(\theta) =  \mathbb{E}_{s,a \sim \rho(.) } [(y_i - Q(s,a;\theta_i))^2]\)
\(\text{Where: } y_i = \mathbb{E}_{s' \sim \epsilon} [r + \gamma max_{a'} Q(s',a';\theta_{i-1}) ]\)
Note that we are using $\theta_{i-1}$ in the notation for $y_i$ in order to update the weights of neural network. This is a very clever approach that they use in the paper. This makes the algorithm more stable. Generating targets using an older set of parameters adds a delay between the time an update to $Q$ is made and the time update affects the target $y_j$. This will make divergence and oscillation more unlikely. In other words predictions are based on parameters of a few steps back rather than the parameters that just have been updated.</p>

<p>Also, unlike supervised learning, here the target value depends on the weights of neural network. In supervised value the target value is known beforehand. Also, $s,a \sim \rho(.)$  is actually the replay memory that we sample from.</p>

<ul>
  <li>Actions are taken with $\epsilon$ greedy! Which means that the agent follows the greedy strategy with probability $(1-\epsilon)$ and the agent selects a random action with probability $\epsilon$ </li>
  <li>Q-learning is a model-free approach. Because we longer know the probability of state transition. If you need more information about DQN is model-free, please watch <a href="https://www.coursera.org/lecture/practical-rl/model-based-vs-model-free-MnfUn">this</a> video.</li>
  <li>Authors in <a href="https://arxiv.org/pdf/1312.5602.pdf">DQN1</a> paper show that there are two ways to show the progress of training. Average $Q$ value and average reward over the course of training. As it turns out, average $Q$ is a better measure of how the agent is learning.</li>
</ul>

<p>Here I am going to talk about a little bit of history. Before using CNN for estimating value function $Q$, there use to be a table for $Q$ in which rows were state and columns were actions.  The update process worked like this:</p>

\[Q_{i+1}(s,a) = \mathbb{E} [r + \gamma max_{a'} Q_{i}(s',a')|s,a ]\]

<p>$Q_i$ Approaches to the optimal value function as $i\rightarrow\infty$. 
The size of the table is $|s| \times |a| $ in which $|s|$ is the total number 
of states and $|a|$ is total number of possible actions that the agent can take. 
This table will get updated at each episode. Doing this is impractical due to the 
large number of states and instead we use a function approximator. In the case of 
DQN, we are using Deep NN in order to perform this where $Q_i(s’,a’)$ is estimated
by CNN and $Q_{i+1}$ is used in the loss function to backpropagate and update the weights.</p>

<p>The <a href="https://storage.googleapis.com/deepmind-media/dqn/DQNNaturePaper.pdf">DQN2</a> 
paper is almost the same as the first one with few differences that I list here:</p>

<ul>
	<li>In the second paper they add t-SNE visualization in order to show hidden representation of image frames. They show that similar image frames end up with similar 2-D representations.</li>
	<li>They use two different networks for target network and prediction network. The parameters of target networks are non-trainable but parameters of prediction network are trainable. At every $C$ iteration, parameters of target network are cloned from prediction network.</li>
</ul>

<p>As mentioned in DQN2 paper, one of the possible future areas of exploration 
could be choosing best memories and discarding memories that do not have sufficient 
information. Right now, they just override the oldest memories with the newest episodes.
What could be done differently? <a href="https://pdfs.semanticscholar.org/6086/ab80f6d2ba77dd7da65c3cadf8cf5fb019ae.pdf">
This</a> work could be a good starting point. One idea could be, after $M$ number of
episodes of training, we could use t-SNE or other dimensionality reduction algorithms
in order to remove similar memories and keep few of them instead of having a lot of
similar experiences.</p>

<h1 id="appendix">Appendix</h1>

<h2 id="experience-replay">Experience Replay</h2>

<p>In DQN papers, they train the network by taking a random sample $(s_t, a, r, s_{t+1})$ from memory. They are doing this for three main reasons.</p>

<p>a. Data efficiency</p>

<p>b. Learning directly from consecutive samples is inefficient due to correlation between samples</p>

<p>c. When learning on-policy, the current parameters determine the next data samples that parameters are trained on. For example, if the maximizing action is to move left, then training samples will be dominated by samples from left-hand side! This is called Experience Replay and it avoids diversion in parameters. The current parameters are different  from those used to generate samples because samples are taken from memory not current experience.</p>

<h2 id="q-learning">Q-learning</h2>

<p>Q-learning is a dynamic programming approach for updating our estimation about agent’s 
  future success. In Q-learning agents experience consists of a sequence of distinct stages 
  or episodes. At the $t^{th}$ times of the $n^{th}$ episode, agent:</p>

<p>Observes its current state $s_t$ 
Selects and performs an action $a_t$ 
Observes the subsequent state $s_{t+1}$ 
Adjusts its $Q_{n-1}$ value using the learning factor $\alpha_{n}$ according to the equation below:</p>

\[Q_{n}(s_t,a) =
\begin{cases}
(1-\alpha_{n})Q_{n-1}(s_t,a) + [r + \gamma V_{n-1}(s_{t+1})], &amp; \text{if not terminal state} \\
Q_{n-1}(s_t,a), &amp; \text{if terminal state}
\end{cases}\]

<p>In this equation $V_{n-1}(s_{t+1})$ is the best possible value that the agent can achieve from state $s_{t+1}$ on ward, in other words expected future reward.</p>

\[V_{n-1}(s_{t+1}) = \max_{b} {Q_{n-1}(s_{t+1},b)}\]

<p>$\alpha_n$ is the learning factor</p>

<p>If $\alpha_n = 0$, agent does not learn and exploits prior information.
If $\alpha_n = 1$, agent learns new things and explores the environment.</p>

<h1 id="references">References</h1>

<div class="footnotes" role="doc-endnotes">
  <ol>
    <li id="fn:1" role="doc-endnote">
      <p><a href="https://storage.googleapis.com/deepmind-media/dqn/DQNNaturePaper.pdf">Human-level control through deep reinforcement learning</a> <a href="#fnref:1" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:2" role="doc-endnote">
      <p><a href="https://arxiv.org/pdf/1312.5602.pdf">Playing Atari with Deep Reinforcement Learning</a> <a href="#fnref:2" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
  </ol>
</div>]]></content><author><name>rozbeh</name><email>rouzbeh.asghari@gmail.com</email></author><category term="private" /><summary type="html"><![CDATA[Deep Reinforcement Learning: DQN]]></summary></entry></feed>